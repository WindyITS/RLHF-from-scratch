# RLHF from Scratch

A Python implementation of Reinforcement Learning from Human Feedback (RLHF) for training language models. This project explores the core concepts and algorithms behind aligning AI systems with human preferences.

**##** Status
ðŸš§ Work in progress - February 2026

**##** Goals
**1.** Implement reward modeling from human preferences
**2.** Implement PPO for language model fine-tuning
**3.** Document learnings and intuitions

**##** Structure
**-**`src/` - Core implementation code
**-**`notebooks/` - Jupyter notebooks for experiments
**-**`data/` - Training data (not tracked in git)

More details coming soon.
**``** ** **And `.gitignore`:** **``
**__**pycache**__**/
*****.pyc
*****.pyo
.ipynb_checkpoints/
data/
*****.pt
*****.pth
*.onnx
wandb/
.env
.DS_Store
